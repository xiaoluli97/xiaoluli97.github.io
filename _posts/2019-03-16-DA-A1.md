---
layout:     post
title:      Brief Introduction to Bias-Variance Trade-off
subtitle:   Assignment 1
date:       2019-03-16
author:     Xiaolu
header-img: img/post-bg-da.jpg
catalog: true
tags:
    - Data Analysis
---
## Motivation
When doing a learning problem, we always want our model to perfectly explain the data, which means that our model not only fits the sample we choose, but also predicts unseen population appropriately. However, it is impossible for us to achieve both goals. Here is an example to intuitively illustrate the dilemma.

![example.png](https://i.loli.net/2019/10/25/g7rf938RxAPiUZX.png)

For the simple one, although not perfectly fit the sample (red points), it still has some predictive power for the underlying population. The complex one does go through every single point in the sample, but almost lose all explanatory power for the out-of-sample data. We call these two extreme cases underfitting and overfitting respectively.

## Bias-Variance Decomposition
To solve the problem mentioned above, we need to first identify those two kinds of errors. Usually we use Mean Square Error to measure the fitness of a model, but it cannot tell us whether the model is overfitting or underfitting.
<img src="https://latex.codecogs.com/gif.latex?MSE=E[(\hat{f}(x)-y)^2]" title="MSE=E[(\hat{f}(x)-y)^2]" />
However, there is a way to divide MSE into three parts, called Bias-Variance Decomposition, which is able to identify the two errors. Here is the derivation: 
<img src="https://latex.codecogs.com/gif.latex?Target&space;function:&space;y=f(x)&plus;e&space;\\Estimation:&space;y=\hat{f}(x)&plus;\epsilon" title="Target function: y=f(x)+e \\Estimation: y=\hat{f}(x)+\epsilon" />
For PDF version [Click here]({{site.baseurl}}/assets/BiasVariance.pdf)



![bias-variance-tradeoff.png](https://i.loli.net/2019/10/25/aQVYPlkg24hB1pw.png)
![bullseye.png](https://i.loli.net/2019/10/25/UXWgnJl38vbK4PV.png)


